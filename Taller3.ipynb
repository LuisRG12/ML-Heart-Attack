{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Heart Attack Analysis & Prediction Dataset"
      ],
      "metadata": {
        "id": "aJxo-lWiezwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar kagglehub si no está instalado\n",
        "!pip install kagglehub\n",
        "\n",
        "# Importar librerías necesarias\n",
        "import pandas as pd\n",
        "import kagglehub"
      ],
      "metadata": {
        "id": "ZJJU8b92be4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWSnjXR7bVIi",
        "outputId": "81242821-7bb2-4023-a405-f31a45dc4961"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/rashikrahmanpritom/heart-attack-analysis-prediction-dataset?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.11k/4.11k [00:00<00:00, 8.01MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Cargar el dataset\n",
        "path = kagglehub.dataset_download(\"rashikrahmanpritom/heart-attack-analysis-prediction-dataset\")\n",
        "file_path = f\"{path}/heart.csv\"\n",
        "data = pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import pandas as pd\n",
        "\n",
        "# Variables categóricas y continuas\n",
        "categorical_cols = [\"sex\", \"cp\", \"fbs\", \"restecg\", \"exng\", \"slp\", \"caa\", \"thall\"]\n",
        "continuous_cols = [\"age\", \"trtbps\", \"chol\", \"thalachh\", \"oldpeak\"]\n",
        "\n",
        "# Separar características y variable de salida\n",
        "X = data.drop(\"output\", axis=1)\n",
        "y = data[\"output\"]\n",
        "\n",
        "# Escalado de variables continuas\n",
        "scaler = StandardScaler()\n",
        "X_scaled_continuous = scaler.fit_transform(data[continuous_cols])\n",
        "\n",
        "# Codificación de variables categóricas\n",
        "encoder = OneHotEncoder(sparse_output=False, drop=\"first\")  # Cambiado de sparse a sparse_output\n",
        "X_encoded_categorical = encoder.fit_transform(data[categorical_cols])\n",
        "\n",
        "# Combinar variables escaladas y codificadas\n",
        "import numpy as np\n",
        "X_prepared = np.hstack((X_scaled_continuous, X_encoded_categorical))\n",
        "\n",
        "# Balanceo de clases con SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_balanced, y_balanced = smote.fit_resample(X_prepared, y)\n",
        "\n",
        "# Dividir los datos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Datos preparados y divididos:\")\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsLLn1w2beKv",
        "outputId": "afe0dd1b-6c30-4a06-8b3a-0f4851ee904c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datos preparados y divididos:\n",
            "X_train shape: (264, 22)\n",
            "y_train shape: (264,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lazypredict\n"
      ],
      "metadata": {
        "id": "8S9BzAufdlDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Comparación de modelos Utilizando Lazypredict"
      ],
      "metadata": {
        "id": "9PxqqzvxeqUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "from lazypredict.Supervised import LazyClassifier\n",
        "from contextlib import redirect_stdout\n",
        "\n",
        "# Redirigir salida estándar\n",
        "with open(os.devnull, \"w\") as fnull:\n",
        "    with redirect_stdout(fnull):\n",
        "        clf = LazyClassifier(verbose=0, ignore_warnings=True)\n",
        "        models, predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
        "\n",
        "# Mostrar resultados\n",
        "print(models)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0ONmd9Zdmby",
        "outputId": "88f20a5c-2ddf-41cf-e788-40ab37ec8206"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 31/31 [00:01<00:00, 19.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\n",
            "Model                                                                           \n",
            "RandomForestClassifier             0.85               0.85     0.85      0.85   \n",
            "NearestCentroid                    0.83               0.83     0.83      0.83   \n",
            "ExtraTreesClassifier               0.82               0.82     0.82      0.82   \n",
            "KNeighborsClassifier               0.82               0.82     0.82      0.82   \n",
            "Perceptron                         0.80               0.80     0.80      0.80   \n",
            "PassiveAggressiveClassifier        0.80               0.80     0.80      0.80   \n",
            "LGBMClassifier                     0.80               0.80     0.80      0.80   \n",
            "LabelSpreading                     0.80               0.80     0.80      0.80   \n",
            "NuSVC                              0.80               0.80     0.80      0.80   \n",
            "LabelPropagation                   0.80               0.80     0.80      0.80   \n",
            "BernoulliNB                        0.80               0.80     0.80      0.80   \n",
            "SVC                                0.80               0.80     0.80      0.80   \n",
            "LogisticRegression                 0.80               0.80     0.80      0.80   \n",
            "SGDClassifier                      0.80               0.80     0.80      0.80   \n",
            "LinearDiscriminantAnalysis         0.80               0.80     0.80      0.80   \n",
            "RidgeClassifierCV                  0.80               0.80     0.80      0.80   \n",
            "RidgeClassifier                    0.80               0.80     0.80      0.80   \n",
            "ExtraTreeClassifier                0.79               0.79     0.79      0.79   \n",
            "XGBClassifier                      0.79               0.79     0.79      0.79   \n",
            "LinearSVC                          0.77               0.77     0.77      0.77   \n",
            "CalibratedClassifierCV             0.77               0.77     0.77      0.77   \n",
            "AdaBoostClassifier                 0.76               0.76     0.76      0.76   \n",
            "DecisionTreeClassifier             0.76               0.76     0.76      0.76   \n",
            "BaggingClassifier                  0.71               0.71     0.71      0.71   \n",
            "GaussianNB                         0.68               0.67     0.67      0.66   \n",
            "QuadraticDiscriminantAnalysis      0.55               0.53     0.53      0.43   \n",
            "DummyClassifier                    0.48               0.50     0.50      0.32   \n",
            "\n",
            "                               Time Taken  \n",
            "Model                                      \n",
            "RandomForestClassifier               0.29  \n",
            "NearestCentroid                      0.02  \n",
            "ExtraTreesClassifier                 0.20  \n",
            "KNeighborsClassifier                 0.03  \n",
            "Perceptron                           0.02  \n",
            "PassiveAggressiveClassifier          0.02  \n",
            "LGBMClassifier                       0.06  \n",
            "LabelSpreading                       0.08  \n",
            "NuSVC                                0.03  \n",
            "LabelPropagation                     0.07  \n",
            "BernoulliNB                          0.02  \n",
            "SVC                                  0.03  \n",
            "LogisticRegression                   0.03  \n",
            "SGDClassifier                        0.02  \n",
            "LinearDiscriminantAnalysis           0.06  \n",
            "RidgeClassifierCV                    0.03  \n",
            "RidgeClassifier                      0.03  \n",
            "ExtraTreeClassifier                  0.03  \n",
            "XGBClassifier                        0.07  \n",
            "LinearSVC                            0.04  \n",
            "CalibratedClassifierCV               0.06  \n",
            "AdaBoostClassifier                   0.16  \n",
            "DecisionTreeClassifier               0.02  \n",
            "BaggingClassifier                    0.06  \n",
            "GaussianNB                           0.02  \n",
            "QuadraticDiscriminantAnalysis        0.02  \n",
            "DummyClassifier                      0.02  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Mejores Modelos\n",
        "| Model                   | Accuracy | Balanced Accuracy | ROC AUC | F1 Score |\n",
        "|-------------------------|----------|--------------------|---------|----------|\n",
        "| RandomForestClassifier  | 0.85     | 0.85               | 0.85    | 0.85     |\n",
        "| NearestCentroid         | 0.83     | 0.83               | 0.83    | 0.83     |\n",
        "| ExtraTreesClassifier    | 0.82     | 0.82               | 0.82    | 0.82     |\n",
        "  "
      ],
      "metadata": {
        "id": "ME6k8opKe65I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Búsqueda de hiperparámetros"
      ],
      "metadata": {
        "id": "PZkgnXbGfyNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar librerías necesarias\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from sklearn.neighbors import NearestCentroid\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Definir los modelos y sus espacios de hiperparámetros\n",
        "models_and_parameters = {\n",
        "    \"RandomForestClassifier\": {\n",
        "        \"model\": RandomForestClassifier(random_state=42),\n",
        "        \"params\": {\n",
        "            \"n_estimators\": [50, 100, 200],\n",
        "            \"max_depth\": [10, 20, None],\n",
        "            \"min_samples_split\": [2, 5, 10]\n",
        "        }\n",
        "    },\n",
        "    \"NearestCentroid\": {\n",
        "        \"model\": NearestCentroid(),\n",
        "        \"params\": {\n",
        "            \"metric\": [\"euclidean\", \"manhattan\"]\n",
        "        }\n",
        "    },\n",
        "    \"ExtraTreesClassifier\": {\n",
        "        \"model\": ExtraTreesClassifier(random_state=42),\n",
        "        \"params\": {\n",
        "            \"n_estimators\": [50, 100, 200],\n",
        "            \"max_depth\": [10, 20, None],\n",
        "            \"min_samples_split\": [2, 5, 10]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Resultados\n",
        "results = {}\n",
        "\n",
        "# Realizar búsqueda de hiperparámetros\n",
        "for name, model_info in models_and_parameters.items():\n",
        "    model = model_info[\"model\"]\n",
        "    params = model_info[\"params\"]\n",
        "    grid_search = GridSearchCV(model, params, cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Guardar los mejores parámetros y el puntaje\n",
        "    results[name] = {\n",
        "        \"Best Params\": grid_search.best_params_,\n",
        "        \"Best Score\": grid_search.best_score_\n",
        "    }\n",
        "\n",
        "# Crear un DataFrame para mostrar los resultados\n",
        "results_df = pd.DataFrame.from_dict(results, orient=\"index\")\n",
        "\n",
        "# Mostrar resultados\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuveUC1MfAaf",
        "outputId": "75a22107-85cf-4c05-8e1b-deb8ad44db22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                              Best Params  \\\n",
            "RandomForestClassifier  {'max_depth': 20, 'min_samples_split': 2, 'n_e...   \n",
            "NearestCentroid                                   {'metric': 'manhattan'}   \n",
            "ExtraTreesClassifier    {'max_depth': 20, 'min_samples_split': 2, 'n_e...   \n",
            "\n",
            "                        Best Score  \n",
            "RandomForestClassifier        0.84  \n",
            "NearestCentroid               0.80  \n",
            "ExtraTreesClassifier          0.84  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Mejor Modelo\n",
        "\n",
        "El mejor modelo fue:\n",
        "\n",
        "RandomForestClassifier        0.84  "
      ],
      "metadata": {
        "id": "1jXz1WrCg-wQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se observa diferencias claras entre utilizar pycaret en el taller pasado y lazypredict en este caso.\n",
        "\n",
        "Los mejores modelos al utilizar pycaret fueron :\n",
        "* Ridge Classifier\n",
        "* Linear Discriminant Analysis\n",
        "* Logistic Regression\n",
        "\n",
        "Mientras que usando lazypredict fueron:\n",
        "* RandomForestClassifier\n",
        "* NearestCentroid\n",
        "* ExtraTreesClassifier\n",
        "\n",
        "Resultados bastante diferentes ya que ambas librerías muestran modelos diferentes entre sí, esto principalmente por la validación cruzada que hace pyCaret respecto a la dicisión simple que hace lazypredict. También pycaret maneja una configuración inicial más óptima loque se refleja en el buen desempeños de algunos modelos. Adicional pycaret hace un ajúste automático de hiperparámatros básicos lo que ayuda a utilizar mejores configuraciones de modelos."
      ],
      "metadata": {
        "id": "7_rRrP-7hG2s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Science Salaries 2023"
      ],
      "metadata": {
        "id": "iMlxeyCXHX7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = pd.read_csv('/content/ds_salaries.csv')"
      ],
      "metadata": {
        "id": "SUE0wHHFJw8U"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargar el dataset\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "# Inspección inicial\n",
        "data.info()\n",
        "\n",
        "# Selección de columnas categóricas y continuas\n",
        "categorical_cols = [\"experience_level\", \"employment_type\", \"job_title\",\n",
        "                    \"salary_currency\", \"employee_residence\", \"company_location\", \"company_size\"]\n",
        "continuous_cols = [\"work_year\", \"salary_in_usd\", \"remote_ratio\"]\n",
        "\n",
        "# Separar características y variable objetivo\n",
        "X = data[categorical_cols + continuous_cols]\n",
        "y = data[\"salary_in_usd\"]\n",
        "\n",
        "# Preprocesamiento\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Crear preprocesador\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), continuous_cols),\n",
        "        (\"cat\", OneHotEncoder(drop=\"first\"), categorical_cols),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Transformar los datos\n",
        "X_preprocessed = preprocessor.fit_transform(X)\n",
        "\n",
        "# Dividir los datos\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Mostrar formas de los conjuntos\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2UNuqfmM3Km",
        "outputId": "d1e22c01-1baa-4944-a410-c5fb19546afb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3755 entries, 0 to 3754\n",
            "Data columns (total 11 columns):\n",
            " #   Column              Non-Null Count  Dtype \n",
            "---  ------              --------------  ----- \n",
            " 0   work_year           3755 non-null   int64 \n",
            " 1   experience_level    3755 non-null   object\n",
            " 2   employment_type     3755 non-null   object\n",
            " 3   job_title           3755 non-null   object\n",
            " 4   salary              3755 non-null   int64 \n",
            " 5   salary_currency     3755 non-null   object\n",
            " 6   salary_in_usd       3755 non-null   int64 \n",
            " 7   employee_residence  3755 non-null   object\n",
            " 8   remote_ratio        3755 non-null   int64 \n",
            " 9   company_location    3755 non-null   object\n",
            " 10  company_size        3755 non-null   object\n",
            "dtypes: int64(4), object(7)\n",
            "memory usage: 322.8+ KB\n",
            "X_train shape: (3004, 270)\n",
            "y_train shape: (3004,)\n"
          ]
        }
      ]
    }
  ]
}